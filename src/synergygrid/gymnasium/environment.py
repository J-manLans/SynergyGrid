import gymnasium as gym
from gymnasium import spaces
from synergygrid.core import GridWorld, AgentAction
from synergygrid.core.resources import BaseResource
from synergygrid.rendering import PygameRenderer
from synergygrid.gymnasium import ObservationHandler


class SynergyGridEnv(gym.Env):
    """
    SynergyGrid reinforcement learning environment.

    A discrete grid-world environment for benchmarking single-agent RL.
    """

    # Metadata required by Gym.
    # "human" for Pygame visualization.
    # FPS caps the render() update rate; each call corresponds to one logic step, not full game fps.
    # Alternative sub-loop in PygameRenderer.render() creates smooth animation between steps.
    metadata = {"render_modes": ["human"], "render_fps": 4}

    # ================= #
    #       Init        #
    # ================= #

    def __init__(
        self,
        max_active_resources: int = 3,
        grid_rows: int = 5,
        grid_cols: int = 5,
        max_steps: int = 100,
        render_mode: str | None = None,
        control: bool = False,
    ):
        # Set up bench environment;

        self._init_vars(
            max_active_resources, grid_rows, grid_cols, render_mode, control
        )
        self._init_world(max_active_resources, grid_rows, grid_cols)
        if self.render_mode == "human":
            self._init_renderer(grid_rows, grid_cols)

        if control:
            self._max_steps = max_steps
            self._human_play_loop()
            return

        # Set up Gymnasium environment:

        # Gymnasium also requires us to define action_space â€” which is the agent's possible
        # actions. Training code can call action_space.sample() to randomly select an action.
        self.action_space = spaces.Discrete(len(AgentAction))

        # Same goes with observation_space: this provides the agent with a structured view
        # of the world that it uses to decide its actions.
        self._observation_handler = ObservationHandler(
            self._world, grid_rows, grid_cols, max_steps
        )
        self.observation_space = self._observation_handler.setup_obs_space()

    # ======================== #
    #    Gymnasium contract    #
    # ======================== #

    def reset(self, *, seed=None, options=None):
        # Gymnasium requires this call to control randomness and reproduce scenarios.
        super().reset(seed=seed)

        # Reset the environment.
        self._observation_handler.reset()
        self._world.reset(self.np_random)

        if self.render_mode == "human":
            self.render()

        obs = self._observation_handler.get_observation()

        # Return observation and info (not used)
        return self._observation_handler.normalize_obs(obs), {}

    def step(self, action: AgentAction):
        # Perform action and adjust variables affected by it
        reward = self._world.perform_agent_action(AgentAction(action))
        self._observation_handler._step_count_down -= 1
        truncated = self._observation_handler._step_count_down <= 0
        terminated = self._world._agent.score <= 0

        if self.render_mode == "human":
            self.render()

        obs = self._observation_handler.get_observation()
        norm_obs = self._observation_handler.normalize_obs(obs)

        # Return observation, reward, terminated, truncated and info (not used)
        return (
            norm_obs,
            reward,
            terminated,
            truncated,
            {},
        )

    def render(self) -> None | str:
        hud_data: dict[str, int] = {}
        hud_data["score"] = self._world._agent.score
        if self.control:
            hud_data["moves"] = self._step_count_down
        else:
            hud_data["moves"] = self._observation_handler._step_count_down
        if len(BaseResource._chained_tiers) > 0:
            if BaseResource._chained_tiers[-1] == self._world.max_tier:
                hud_data["current tier chain"] = 0
            else:
                hud_data["current tier chain"] = BaseResource._chained_tiers[-1]
        else:
            hud_data["current tier chain"] = 0

        return self._renderer.render(
            self._world._agent.position,
            self._world.get_resource_is_active_status(True),
            self._world.get_resource_positions(True),
            self._world.get_resource_meta(True),
            hud_data,
        )

    # ================== #
    #       Helpers      #
    # ================== #

    # === Init === #

    def _init_vars(
        self,
        max_active_resources: int,
        grid_rows: int,
        grid_cols: int,
        render_mode: str | None,
        control: bool,
    ) -> None:
        self.max_active_resources = max_active_resources
        self.grid_rows = grid_rows
        self.grid_cols = grid_cols
        self.render_mode = render_mode
        self.control = control

    def _init_world(
        self, max_active_resources: int, grid_rows: int, grid_cols: int
    ) -> None:
        self._world = GridWorld(
            max_active_resources, grid_rows=grid_rows, grid_cols=grid_cols
        )

    def _init_renderer(self, grid_rows: int, grid_cols: int) -> None:
        self._renderer = PygameRenderer(
            grid_rows=grid_rows,
            grid_cols=grid_cols,
            fps=self.metadata["render_fps"],
        )

    def _human_play_loop(self):
        self._step_count_down = 100
        self._renderer._step_fps = 60
        self._world.reset()
        action = str(self.render()).upper()

        while True:
            if not action == "NONE":
                agent_action = AgentAction[action]
                self._world.perform_agent_action(agent_action)
                self._step_count_down -= 1
                truncated = self._step_count_down <= 0
                terminated = self._world._agent.score <= 0

                if terminated or truncated:
                    break

            action = str(self.render()).upper()
